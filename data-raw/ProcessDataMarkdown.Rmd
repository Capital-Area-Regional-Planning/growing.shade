---
title: "Growing Shade methods"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  github_document:
    toc: true
always_allow_html: true
urlcolor: blue
---

The Growing Shade Project couples remote sensing data and demographic data to create insights about the intersection between the natural environment and people in real-time. 

Becuase there are several steps which must be done in order, this document lays those steps out logically. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F)
library(tidyverse)
library(tigris)
library(sf)

```

# Create and export files for GEE

Google Earth Engine needs some assets created. 

```{r gee_assets}
######
# Create a gridded area across the region to calibrate sentinel tree cover with 1m2 land cover tree data
#####
wholearea <- metc_region %>%
  summarise(st_union(.))

# make a equal area grid; there are 704 tracts, so I want to make at least 1000 grids I think?
g = st_make_grid(wholearea,
                 n = c(36, 36)) %>% 
  st_intersection(wholearea) 

geometry = st_sfc(lapply(1:length(g), function(x) st_geometrycollection()))
df <- st_sf(id = 1:length(g), geometry = g)

# ggplot() +
#   geom_sf(data = wholearea) +
#   geom_sf(data = df,
#           fill = "transparent")

sf::st_write(df, "/Users/escheh/Documents/GitHub/planting.shade/storymap-info/shapefiles/metc_grid.shp", append = FALSE)



```


# Google Earth Engine

Need to run some things, export some things etc.

# Calibrate tree canopy coverage

For some good reasons we are going to use data from 2020 for the tree canopy coverage. But since Sentinel has a 10 meter squared pixel size, this is larger than a lot of trees. In exploring the data, it appears as if the canopy coverage from Sentinel is about two times higher than what it should be (based on aerial imagery). However, this does make sense, because Sentinel is likely saying that a 10x10m pixel has a tree, even if that tree doesn't cover the whole area. In essence, it's telling us where land is covered at least half by trees. But we can refine this further!! Let's calibrate with the outdated UMN 1 meter squared land use file. 


```{r calibrate_trees}
calibrate_trees <- read_csv("../data-raw/UMNTreeAcres_metcgrid_year2020.csv") %>%
  rename(umn = `1`) %>%
  full_join(read_csv("../data-raw/TreeAcres_metcgrid_year2020.csv") %>%
              rename(sentinel = `1`),
            by = 'id')

# cor.test(~ umn + sentinel,
#          data = calibrate_trees)
# cor.test(~ umn + I(sentinel^2),
#          data = calibrate_trees)

calibrate_lm <- (lm(umn ~ sentinel, data = calibrate_trees))
calibrate_lm2 <- (lm(umn ~ 0 + sentinel, data = calibrate_trees))
calibrate_lm3 <- (lm(umn ~ I(sentinel ^ 2), data = calibrate_trees))
calibrate_lm4 <- (lm(log(umn) ~ sentinel, data = calibrate_trees))
anova(calibrate_lm, calibrate_lm2, calibrate_lm3, calibrate_lm4) # the middle model is best!

summary(calibrate_lm2)$r.squared # r2
summary(calibrate_lm2)$coefficients[,4] # p-value

calib_coeff <- summary(calibrate_lm2)$coefficients[,1] # coefficient

calibrate_trees %>%
  ggplot(aes(x = (umn), y = (sentinel * calib_coeff))) +
  geom_point(alpha = .5) +
  geom_abline(slope=1, intercept=-25, col = 'blue', lwd = 2) +
  # geom_smooth(method = "lm", fill = NA)+
  theme_minimal()  + 
  labs( x = "UMN tree acres in grid", y = "Sentinel acres in grid",
        title = "calibrated")
  # labs(y = expression ("Sentinel in"~m/s^2))

```


# Make geometry shapefiles and crosswalks

Since we're going to be making a map which shows census tracts, ctus, or neihborhoods depending on the user imput, 

```{r geos}
# fxns to make easy -----
st_erase = function(x, y) st_difference(x, st_union(st_combine(y)))

# find centroid of geographies
find_centroid <- function(x, ...) {
  points <- x %>%
    mutate(zoom = case_when(Shape_Area < 1e6 ~ 15,
                            Shape_Area < 1e8 ~ 13,
                            Shape_Area < 1e9 ~ 12,
                            TRUE ~ 11)) %>%
    st_transform(26915) %>%
    st_centroid() %>%
    st_transform(4326) %>%
    select(!!!quos(...), geometry, zoom) %>%
    mutate(lat = unlist(map(.$geometry,1)),
           long = unlist(map(.$geometry,2))) %>%
    sf::st_drop_geometry()
  geos <- x %>%
    select(!!!quos(...), city, geometry) %>%
    st_transform(4326)
  combo <- full_join(geos, points) %>%
    arrange(!!!(quos(...))) 
  return(combo)
}

# river layer to erase Mississippi and Minnesota rivers-------
temp <- tempfile()
download.file("ftp://ftp.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_metc/water_lakes_rivers/gpkg_water_lakes_rivers.zip", destfile = temp)
river_lake_buffer <- sf::read_sf(unzip(temp, "water_lakes_rivers.gpkg")) %>% 
  filter(NAME_DNR %in% c("Mississippi", "Minnesota")) #%>% #these rivers are boundaries
  # st_buffer(200) %>% #add 10m buffer
  # # st_simplify(dTolerance = 100) %>%
  # # st_buffer(300) %>%
  # st_union() %>% st_buffer(0) 


########
# geos
######
# tracts------
mn_tracts_1 <- tigris::tracts(state = "MN",
                            county = c("Anoka", "Carver", "Dakota", "Hennepin", "Ramsey", "Scott", "Washington"))%>%
  sf::st_transform(4326) %>%
  mutate(GEO_NAME = GEOID)


### neighborhoods -----------
#st paul here: https://information.stpaul.gov/City-Administration/District-Council-Shapefile-Map/dq4n-yj8b
#minneap here: https://opendata.minneapolismn.gov/datasets/communities/explore?location=44.970861%2C-93.261718%2C12.85
#brooklyn park here: but no dl: https://gis.brooklynpark.org/neighborhoodinfo/

minneap <- read_sf("../data-raw/minneapolis communities/Minneapolis_Communities.shp") %>%
  rename(GEO_NAME = CommName) %>%
  mutate(Shape_Area = as.numeric(st_area(.))) %>%
  mutate(city = "Minneapolis")

stpaul <- read_sf("../data-raw/stpaul communities/geo_export_0c076f52-d6ff-4546-b9fa-bd9980de6e8a.shp") %>%
  mutate(Shape_Area = as.numeric(st_area(.))) %>%
  rename(GEO_NAME = name2) %>%
  mutate(city = "St. Paul")

nhood_geo <- bind_rows(minneap, stpaul) %>%
  find_centroid(., GEO_NAME)


#### ctus -----------
temp <- tempfile()
temp2 <- tempfile()
download.file(
  "https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_metc/bdry_metro_counties_and_ctus/shp_bdry_metro_counties_and_ctus.zip",
  destfile = temp
)
unzip(zipfile = temp, exdir = temp2)
list.files(temp2)

ctu_geo <- sf::read_sf(paste0(temp2, pattern = "/CTUs.shp")) %>%
  select(CTU_NAME, Shape_Area) %>%
  add_column(city = "doesn't matter") %>%
  find_centroid(., CTU_NAME) %>%
  select(-city) %>%
  rename(GEO_NAME = CTU_NAME) 

files <- list.files(temp2, full.names = T)
file.remove(files)



###########
# link tracts to ctus and nhoods
#########
# find crosswalks
find_crosswalks <- function(x) {
  crosswalk <- x %>%
  st_transform(26915) %>%
  st_buffer(-150) %>% #buffer the perimeter of the geography
  st_erase(river_lake_buffer %>%
             st_buffer(200) %>% #buffer out rivers
             st_union() %>% 
             st_buffer(0)) %>% 
  st_intersection(mn_tracts_1 %>% 
                    select(GEOID) %>%
                    rename(tract_id = GEOID) %>%
                    st_transform(26915)) %>%
  st_drop_geometry()
    
  return(crosswalk)
}

ctu_crosswalk <- find_crosswalks(ctu_geo)
nhood_crosswalk <- find_crosswalks(nhood_geo)

test <- "Hilltop"
test <- "Landfall"
test <- "South St. Paul"
mn_tracts_1 %>%
  right_join(ctu_crosswalk %>% filter(GEO_NAME == test),
             by = c("GEOID" = "tract_id")) %>%
  ggplot()+
  geom_sf() +
  geom_sf(data = filter(ctu_geo, GEO_NAME ==test), fill = NA, color = "blue")

test <- "The Greater East Side"
mn_tracts_1 %>%
  right_join(nhood_crosswalk %>% filter(GEO_NAME == test),
             by = c("GEOID" = "tract_id")) %>%
  ggplot()+
  geom_sf() +
  geom_sf(data = filter(nhood_geo, GEO_NAME ==test), fill = NA, color = "blue")



usethis::use_data(ctu_crosswalk, overwrite = TRUE)
usethis::use_data(nhood_crosswalk, overwrite = TRUE)

wide_ctu_crosswalk_1 <- ctu_crosswalk %>%
  group_by(tract_id) %>%
  count() %>%
  left_join(ctu_crosswalk) %>%
  add_column(cities = 999) %>%
  select(tract_id, GEO_NAME, cities, n) %>%
  # spread(cities, GEO_NAME)
  pivot_wider(names_from = cities, values_from = GEO_NAME, values_fn = list) %>%
  unnest_wider(`999`) 

wide_ctu_crosswalk <- wide_ctu_crosswalk_1 %>%
  mutate(jurisdiction = paste(`...1`, `...2`, `...3`, `...4`, `...5`, `...6`, `...7`, sep = ", ")) %>%
  select(tract_id, jurisdiction) %>%
  mutate(jurisdiction = str_replace(jurisdiction, ", NA", ""),
         jurisdiction = str_replace(jurisdiction, ", NA", ""),
         jurisdiction = str_replace(jurisdiction, ", NA", ""),
         jurisdiction = str_replace(jurisdiction, ", NA", ""),
         jurisdiction = str_replace(jurisdiction, ", NA", ""),
         jurisdiction = str_replace(jurisdiction, ", NA", ""),
         jurisdiction = str_replace(jurisdiction, ", NA", "")) %>%
  rename(GEOID = tract_id)
usethis::use_data(wide_ctu_crosswalk, overwrite = TRUE)


mn_tracts <- mn_tracts_1 %>%
  full_join(wide_ctu_crosswalk)
usethis::use_data(mn_tracts, overwrite = TRUE)



```

# Process GEE code

```{r gee}
# GEE data is in repo "users/ehe/MetCoucil/GrowingShade_CanopyCoverage"
# https://code.earthengine.google.com/a0da66053ecb26b668df4297c4ebed59

# ndvi
tract_ndvi <- read_csv("../data-raw/meanNDVI_tracts_year2020.csv",
                       col_types = cols(GEOID10 = "c", `system:index` = "c", Year = 'd', ndvi = 'd', `.geo` = 'c')) %>%
  select(-`system:index`, -.geo, -Year)

# canopy coverage
canopy <- read_csv("../data-raw/TreeAcres_tracts_year2020.csv",
                   col_types = cols(.default = "d", GEOID10 = "c")) %>%
  left_join(sf::st_drop_geometry(mn_tracts_1), by = c("GEOID10" = "GEOID")) %>%
  transmute(GEOID10 = GEOID10, 
            treeacres = `1`,
            landacres = ALAND / 4046.86,
            canopy_percent = treeacres / landacres * calib_coeff) #use calibration coefficeint

canopy
# # tree raster
# treecrs <- raster::raster("./data/TreeMap_crs4326_2020.tif")
# treecrs
# 
# cuts=c(0, 1) #set breaks
# pal <- colorRampPalette(c("white","green"))
# plot((treecrs %>% crop(filter(ctu_list, GEO_NAME == "Afton"))), breaks=cuts, col = pal(7)) #plot with defined breaks
# plot((treecrs %>% crop(filter(ctu_list, GEO_NAME == "Afton")))) #plot with defined breaks


# # I did this to crop NDVI, but I'm not using NDVI anymore
# test <- reclassify(treecrs, cbind(-Inf, .5, NA), right=FALSE) 
# raster::writeRaster(test, './data/tree_raster.tif', overwrite=TRUE)

# test <- raster(x = "./data/BinaryTreeMap_crs4326_2020.tif") %>% #this is from -999 to 1; not so good
#   crop(filter(ctu_list, GEO_NAME == "Lake Elmo"))
# plot(test)
```


# link canopy coverage with geographies
```{r}
tree_summary <- function(x) {
  x %>%
    left_join(canopy %>%
                rename(tract_id = GEOID10),
              by = "tract_id") %>%
    # st_drop_geometry() %>%
    group_by(GEO_NAME) %>%
    summarise(min = round(min(canopy_percent)*100, 1),
              max = round(max(canopy_percent)*100, 1),
              ntracts = n())
}


nhood_list <- nhood_geo %>%
  full_join(tree_summary(nhood_crosswalk)) %>%
  arrange(city, GEO_NAME) %>%
  st_transform(4326)

usethis::use_data(nhood_list, overwrite = TRUE)


ctu_list <- ctu_geo %>%
  full_join(tree_summary(ctu_crosswalk)) %>%
  arrange(GEO_NAME) %>%
  st_transform(4326)

usethis::use_data(ctu_list, overwrite = TRUE)

```



# Demographic variables and rescale

```{r demographics}
###################
# download equity considerations dataset
###################
temp <- tempfile()
download.file("https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_metc/society_equity_considerations/xlsx_society_equity_considerations.zip",
  destfile = temp
)

equity <- readxl::read_xlsx(unzip(temp, "EquityConsiderations_Full.xlsx")) %>%
  janitor::clean_names() 

fs::file_delete("EquityConsiderations_Full.xlsx")


## --------------variables of interest from equity considerations
equity_data_raw <- equity %>%
  select(tr10, 
         ppov185,
         prim_flood,
         pwhitenh,
         p_0017,
         p_65up,
         avg_temp,
         phhi_qntl1,
         green_roof,
         env_cancer,
         luse_green,
         tr_ej,
         holc_pred,
         mdhhincnow,
         pd_any,
         pblacknh,
         pasiannh,
         phisppop,
         pamindnh,
         pwk_nowork,
         pownhome) %>%
  rowwise() %>%
  mutate(luse_notgreen = 1 - luse_green,
         pbipoc = 1 - pwhitenh,
         holc_pred = if_else(is.na(holc_pred), 0, holc_pred),
         sens_age = p_0017 + p_65up) %>% #"mutate" reformats any variables that need it
  select(-luse_notgreen,#and then I want to remove the variable I don't need anymore
         -pwhitenh) 


##########
# CDC health data
#########
# variable options are documented here: https://www.cdc.gov/places/measure-definitions/index.html
# api token: https://chronicdata.cdc.gov/profile/edit/developer_settings

library("RSocrata")
#metadata https://dev.socrata.com/foundry/chronicdata.cdc.gov/cwsq-ngmh

health <- read.socrata(
  "https://chronicdata.cdc.gov/resource/cwsq-ngmh.json?$where=countyfips in('27003', '27019', '27037', '27053', '27123', '27139', '27163')",
  app_token = "D1kEEJEDVBpDppDIdDmwNXeVT",
  email     = "ellen.esch@metc.state.mn.us",
  password  ="TQfY5%Q3xY") %>%
  rename(GEOID10 = locationname)

health
names(health)
levels(as.factor(health$measure))

health2 <- health %>%
  filter(measure %in% c("Current asthma among adults aged >=18 years",
                        "Chronic obstructive pulmonary disease among adults aged >=18 years",
                        "Mental health not good for >=14 days among adults aged >=18 years",
                        "Physical health not good for >=14 days among adults aged >=18 years")) %>%
  select(GEOID10, measureid, data_value) %>%
  mutate(data_value = as.numeric(data_value) / 100) %>% #change to fraction
  pivot_wider(names_from = measureid, values_from = data_value) %>%
  rename(tr10 = GEOID10)

###################
# combine data sources
###################

eva_data_raw <- equity_data_raw %>% 
  full_join(canopy %>% rename(tr10 = GEOID10)) %>%
  full_join(tract_ndvi %>% rename(tr10 = GEOID10)) %>%
  full_join(health2) %>%
  mutate(ndvi2 = ndvi,
         canopy_percent2 = canopy_percent) %>%
  rename(tract_string = tr10) #and for this project, I need to rename the tract variable

###################
# add some human-readable metadata
###################

## -------------------------------describe data
#cc (climate change preset) = prim_flood, avg_temp, ndvi
#ej (environmental justice preset) = pbipoc, phhi_qntl1, prim_flood, avg_temp, ndvi
#ph (public health preset)
eva_data_codes <- tribble(~variable, ~name, ~type, ~interpret_high_value, ~cc, ~ej, ~ph, ~cons,
                          "ppov185",	"% people with income <185% of the poverty threshold", "people", "high_opportunity", 0, 1, 0, 0,
                          "prim_flood", "% developed acres in primary flood zone", "environment", "high_opportunity", 1, 1, 0, 0,
                          "pbipoc", "% people of color", "people", "high_opportunity", 0, 1, 0, 0,
                          "p_0017", "% people age 17 or younger", "people",  "high_opportunity", 0, 0, 0, 0, 
                          "p_65up", "% people age 65 or older", "people",  "high_opportunity", 0, 0, 0,  0,
                          "avg_temp", "Land surface temp on hot summer day", "environment",  "high_opportunity", 1, 1, 1, 0,
                          # "phhi_qntl1", "% households with annual income less than $35,000 (bottom quintile of households)", "people",  "high_opportunity", 0, 1, 0, 0,
                          # "green_roof", "Water holding potential of green roofs on commercial bldgs", "environment",  "high_opportunity", 
                          "env_cancer", "Lifetime cancer risk from air toxics", "people", "high_opportunity", 0, 1, 1,  0,
                          # "luse_notgreen", "% of tract NOT used for green space", "environment", "high_opportunity"
                          "ndvi", "Average greenness (2020 NDVI)", "tree", "low_opportunity", 1, 0, 1,  0,
                          "ndvi2", "Average greenness (2020 NDVI) - for conservation", "tree", "high_opportunity", 0, 0, 0, 1,
                          "tr_ej", "Area of Environmental Justice Concern", "people", "high_opportunity", 0, 1, 0, 0,
                          "holc_pred", "Share of tract's land acreage redlined", "people", "high_opportunity", 0, 1, 0, 0,
                          "canopy_percent", "% tree canopy coverage in 2020", "tree", "low_opportunity", 1, 0, 1, 0,
                          "canopy_percent2", "% tree canopy coverage in 2020 - for conservation", "tree", "high_opportunity", 0, 0, 0, 1,
                          "mdhhincnow", "Median household income, 2015-2019 period (in 2019 dollars)", "people", "low_opportunity", 0, 0, 0, 0,
                          "sens_age", "% people 17 or younger and 65 or older", "people", "high_opportunity", 0,0,1,0,
                          "pd_any", "% people with any disability", "people", "high_opportunity", 0, 0, 0, 0,
                          "pblacknh", "% residents who identify as Black or African American, non-Latino", "people", "high_opportunity", 0, 0, 0, 0,
                          "pasiannh", "% residents who identify as Asian, non-Latino", "people", "high_opportunity", 0, 0, 0, 0,
                          "phisppop", "% residents who identify as Hispanic or Latino", "people", "high_opportunity", 0, 0, 0, 0,
                          "pamindnh", "% residents who identify as Indigenous, non-Latino", "people", "high_opportunity", 0, 0, 0, 0,
                          "pwk_nowork", "% of residents age 16-64 who did not work in past 12 months", "people", "high_opportunity", 0, 0, 0, 0,
                          "pownhome", "% of residents who own their home", "people", "high_opportunity", 0,0,0,0,
                          "MHLTH", "Mental health not good for >=14 days among adults aged >=18 years (%)", "people", "high_opportunity", 0,0,0,0,
                          "PHLTH", "Physical health not good for >=14 days among adults aged >=18 years (%)", "people", "high_opportunity", 0,0,0,0,
                          "COPD", "Chronic obstructive pulmonary disease among adults aged >=18 years (%)", "people", "high_opportunity", 0,0,0,0,
                          "CASTHMA", "Current asthma among adults aged >=18 years (%)", "people", "high_opportunity", 0,0,0,0
                          )
eva_data_codes %>% filter(ph == 1)

###################
# #create final dataset - no spatial data here
# #note: spatial data should be joined after any summarizing is done to save some computation time
###################

# #long data
eva_data_main <- eva_data_raw %>%
  pivot_longer(names_to = "variable", values_to = "raw_value", -tract_string) %>% #end the code after this line if you just want the reshaped data
  group_by(variable) %>%
  mutate(MEAN = mean(raw_value, na.rm = T),
         SD = sd(raw_value, na.rm = T),
         MIN = min(raw_value, na.rm = T),
         MAX = max(raw_value, na.rm = T),
         COUNT = as.numeric(sum(!is.na(raw_value))),
         z_score = (raw_value - MEAN)/SD) %>%
  
  right_join(eva_data_codes, by = 'variable') %>%
  
  # #we want high opportunity to be a high value, so this reorders those values if needed
  # mutate(opportunity_zscore = case_when(interpret_high_value == "high_opportunity" ~ z_score,
  #                                       interpret_high_value == "low_opportunity" ~ z_score * (-1),
  #                                         TRUE ~ NA_real_)) %>%
  
  #create nominal weights
  mutate(weights_nominal = case_when(interpret_high_value == "high_opportunity" ~ (raw_value - MIN) / (MAX - MIN) * 10,
                                     interpret_high_value == "low_opportunity" ~ 10 - (raw_value - MIN) / (MAX - MIN) * 10,
                                     TRUE ~ NA_real_)) %>%
  
  #Weights Standard Score
  mutate(weights_scaled = case_when(interpret_high_value == "high_opportunity" ~ pnorm(z_score) * 10,
                                    interpret_high_value == "low_opportunity" ~ (10 - pnorm(z_score) * 10),
                                    TRUE ~ NA_real_)) %>%
  
  #weights rank
  mutate(weights_rank = case_when(interpret_high_value == "high_opportunity" ~ min_rank(desc(weights_nominal)) / COUNT * 10,
                                  interpret_high_value == "low_opportunity" ~ min_rank(desc(weights_nominal)) / COUNT * 10,
                                  TRUE ~ NA_real_)) %>%
  
  # #rank
  mutate(overall_rank = case_when(interpret_high_value == "high_opportunity" ~ min_rank(desc(as.numeric(weights_nominal))),
                                  interpret_high_value == "low_opportunity" ~ min_rank(desc(as.numeric(weights_nominal))))) %>%
  # 
  #clean
  select(-MEAN, -SD, -MIN, -MAX)  %>%
  full_join(wide_ctu_crosswalk %>% rename(tract_string = GEOID))

########
# save data
########

#this works if you're in a package
usethis::use_data(eva_data_main, overwrite = TRUE)

#otherwise use this
# write_csv(eva_data_main, "./eva_data_main.csv")

# ########
# # create metadata
# #########
md1 <- eva_data_main %>% group_by(variable) %>% summarise(MEANRAW = mean(raw_value, na.rm = T),
                                                          MEANSCALED = mean(weights_scaled, na.rm = T))
metadata <- eva_data_main %>%
  dplyr::group_by(type, name, variable, interpret_high_value, cc, ej, ph, cons) %>%
  dplyr::count() %>%
  dplyr::ungroup() %>%
  full_join(md1)

usethis::use_data(metadata, overwrite = TRUE)

```






