---
title: "Update Script"
author: "Liz Levy"
date: "2023-09-18"
output: html_document
---
# Introduction

This script should be run annually every January or February to update the Dane County Growing Shade website. Growing Shade relies on multiple external data sets which are updated and released at different times of the year. To simplify things, it's easier to just perform one annual update. However, you can also use this script to update just one or two data sets.

The data set that is released the latest is USDAâ€™s NASS CDL, which is required to separated canopy from cropland. The most recent 2022 layer was released on January 30, 2023, so it is expected that future releases will also be in late January. 

Note that this script is for updates, not for building the Growing Shade site from scratch. Much of the code is similar to the 01tutorial, 02geography, and 03remotesensing script, but it relies on files that have already been produced/downloaded from running these scripts.

*Before running this script, check if the new CDL layer has been released here*:  https://www.nass.usda.gov/Research_and_Science/Cropland/SARS1a.php

Run the chunk below to load in all the required libraries.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = F, message = F, warning = F,
  cache = F, progress = F
)

library(dplyr)
library(tidyr)
library(readr)
library(stringr)
library(tibble)
library(ggplot2)
library(tigris)
library(sf)
library(tidycensus)
library(ggbeeswarm)
library(RSocrata)
library(here)
library(arcpullr)
library(terra)

#utility function to filter out the block groups that are entirely lake Monona or Mendota
remove_lake_bgs <- function(df, colName) {
   filter(df, !(!!as.name(colName) %in% c("550259917030", "550259917020")))
}
```

# Set Parameters

These data sets are updated and released annually (at minimum). If you don't want to include the newest version of a data set, set its parameter to FALSE - this is useful if you are adding in updated information outside of the normal annual update. 

```{r set-parameters-annual}

american_community_survey <- TRUE
places_health_data <- TRUE
cities_towns_unincorporated <- TRUE
neighborhoods <- FALSE
canopy <- FALSE
NDVI <- FALSE
primary_floodplains <- FALSE #TODO - is it annual?
ejscreen <- FALSE
```

The decennial census is updated every 10 years. The next release is expected in *April of 2030*. You can check on the status here...

The DC landuse classification updated every ? years. The next release is expected in... You can check on the status here...

The temperature parameter refers to the average land surface temperature during a heat wave. The data comes from three heat waves: 2012-07-01 to 2012-07-07, 2016-07-21 to 2016-07-24, and 2023-07-25 to 2023-07-28. You may wish to occasionally update this data to include more recent data.

TODO - if decennial census is new, so new block groups, does that mean everything needs to be reprocessed? I think so... probably add in an override

```{r set-parameters-intermittent}

decennial_census <- TRUE
landuse <- FALSE
temperature <- TRUE
```


A few more parameters. These geography parameters don't need to be changed unless you're running the script for a different region than Dane County.
```{r set-parameters-other}
#What year is the new data from?
acs_year <- 2022
decennial_year <- 2020

#geography parameters
state <- c("WI")
county <- c("Dane")
```


Simply go through the rest of the script step by step, following any instructions then running each code chunk.

# Decennial Census

The decennial census data is automatically downloaded.

You may need an API key from Census:

-   [Request an api key](https://api.census.gov/data/key_signup.html)
-   Enter in the console: `usethis::edit_r_environ()`
-   When the `.Renviron` file comes up in the editor, type: `CENSUS_KEY="KEY GOES HERE, INSIDE QUOTES"`
-   Save and close the `.Renviron` file.
-   Restart R.

TODO - put something in about this, some kind of toggle like in tutorial script? - Race variables come from the decennial 2020 census. Currently, the decennial census data is preferred over ACS data because it is a population count rather than a sample and because ACS data uses the average of 5 years (so starting with the ACS 2021-2025 data, it may be logical to switch over to using the ACS data instead

TODO - add code to download current blockgroup shapefile if decennial has been updated, along with reprocessing the historic data

TODO - deal with potentially needing to download a new crosswalk between old and new blockgroups

```{r decennial-census}

if (decennial_census == TRUE) {


  census_api_key(Sys.getenv("CENSUS_KEY"))

  #the variables to pull
  decennial_var <- c(
    "P2_005N", # white nh
    "P2_006N", # black nh
    "P2_007N", # amin nh
    "P2_008N", # asian nh
    "P2_009N", # hawaiian pi nh
    "P2_010N", # some other nh
    "P2_011N", # 2+ nh
    "P2_002N", # hispanic

    "H1_001N" # total housing units
  )

  #get the data
  decennial_data <- get_decennial(
    geography = "block group",
    variables = decennial_var,
    summary_var = "P1_001N",
    year = decennial_year,
    state = state,
    county = county,
    geometry = F
  ) %>%
    pivot_wider(names_from = variable, values_from = value) %>%
    # process data
    mutate(across(starts_with("P2"), ~ .x / summary_value * 100)) %>%
    rename(
      whitenh = P2_005N,
      pblacknh = P2_006N,
      pamindnh = P2_007N,
      phisppop = P2_002N,
      pasiannh = P2_008N,
      ppacificinh = P2_009N,
      housing_units = H1_001N
    ) %>%
    mutate(
      pothmultnh = (P2_010N + P2_011N),
      pbipoc = 100 - whitenh,
      population_count = summary_value
    ) %>%
    select(-c(starts_with("P2"), summary_value)) %>%
    # get a nice name for tracts
    separate(NAME, into = c("bg", "tract", "county", "state"), sep = ",") %>%
    mutate(fancyname = paste0(county, " tract ", str_remove(tract, "Census Tract "), ", block group ", str_remove(bg, "Block Group "))) %>%
    select(-bg, -tract, -county, -state) %>% 
    remove_lake_bgs("GEOID")


  #get regional averages
  decennial_metadata <- get_decennial(
    geography = "county",
    variables = decennial_var,
    summary_var = "P1_001N",
    year = decennial_year,
    state = state,
    county = county,
    geometry = F
  ) %>%
    add_column(geo = "region averages") %>%
    group_by(geo, variable) %>%
    summarise(
      value = sum(value),
      summary_value = sum(summary_value)
    ) %>%
    pivot_wider(names_from = variable, values_from = value) %>%
    mutate(across(starts_with("P2"), ~ .x / summary_value)) %>%
    rename(
      whitenh = P2_005N,
      pblacknh = P2_006N,
      pamindnh = P2_007N,
      phisppop = P2_002N,
      housing_units = H1_001N
    ) %>%
    mutate(
      pothmultnh = (P2_010N + P2_011N),
      pasiannh = (P2_008N + P2_009N),
      pbipoc = 1 - whitenh,
      population_count = summary_value
    ) %>%
    select(-c(starts_with("P2"), summary_value, geo))
    
  
  save(decennial_data, decennial_metadata, file = paste0(here::here(), "/data-raw/decennial_data.rda"))

} else {
  load(paste0(here::here(), "/data-raw/decennial_data.rda"))
}
```



# American Community Survey

The ACS data is automatically downloaded. If you haven't already entered the API information, you may need to follow the API instructions for the decennial census.

ACS data may not be valid county-wide depending on the margin of error. Test each variable for validity first using the 
City of Madison Data Team's method and guidance on combining MOEs from the Census Bureau   https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch08.pdf

If you want to add in any new variables from the ACS, make sure to check that they are valid. If a variable is not valid,
it should not be used in Growing Shade. Margins of error will change year-to-year, so be sure to test all variables
whenever the ACS data is updated.

```{r}
#testing for ACS variable validity
if (american_community_survey == TRUE) {
  census_api_key(Sys.getenv("CENSUS_KEY"))
  
 v20 <- load_variables(acs_year, "acs5", cache = TRUE)
  # View(v20 %>% filter(geography == "tract"))
  # View(v20 %>% filter(geography == "block group"))
  #  you may need to access the table shells: https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.html
  #  census reporter topics are also very useful! https://censusreporter.org/topics/

  #the list of variables to grab
  acs_variables <- c(
    "B01001_001", # poptotal
    paste0("B01001_00", c(3:6)), # under18 m
    paste0("B01001_0", c(27:30)), # under18 f
    paste0("B01001_0", c(20:25, 44:49)), # over 65m, f
    "B19013_001", # median hh income
    "B25003_001", # tenure_total
    "B25003_002", # tenure owners
    "B23025_001", # employment status denominator
    "B23025_007", # unemployed
    "B17017_031", # all HH above poverty
    #"B17017_032", # all family HH above poverty
    #"B17017_003", # all family hh below poverty
    paste0("B19001_00", c(2:9)), # total households earning below 50k
    "B19001_010", # total households earning below 50k
    "B17017_001" # total households
  )
  
  #function to get acs variables
    acs_fxn <- function(.geo, .process = NULL, .extra_vars = NULL) {
    step1 <- get_acs(
      geography = .geo,
      variables = if (is.null(.extra_vars)) {
        acs_variables
      } else {
        .extra_vars
      }, # c(acs_variables, .extra_vars),
      survey = "acs5",
      state = state,
      county = county,
      year = acs_year
    ) #%>%
      # pivot_wider(names_from = variable, values_from = estimate)
    return(step1)
    }
    
  #transform the variable estimates and moes
  acs_data_step1 <- acs_fxn(.geo = "block group") %>%
    mutate(tract_id = substr(GEOID, start = 1, stop = 11)) %>% 
    remove_lake_bgs("GEOID")
    
    
  #TODO - should I be dropping NAs? It does change the #s slightly...
  #check if the variables are valid according to the method from the City of Madison Data Team
  check_validity <- function(num, denom, name) {
    
    #proportion
    if(!is.na(denom[1])) {
      test_num <- filter(acs_data_step1, variable %in% num) %>% 
        group_by(GEOID) %>% 
        summarise(estimate=sum(estimate), moe=moe_sum(moe, estimate))

      test_denom <- filter(acs_data_step1, variable %in% denom) %>% 
        group_by(GEOID) %>% 
        summarise(estimate=sum(estimate), moe=moe_sum(moe, estimate))
      
      test <- left_join(test_num, test_denom, by = c("GEOID")) %>% 
        mutate(moe_prop = moe_prop(estimate.x, estimate.y, moe.x, moe.y)*100) %>% 
        mutate(estimate_prop = estimate.x / estimate.y*100) %>% 
        mutate(cv = case_when(estimate_prop == 0 | is.nan(estimate_prop) ~ 100,
                              TRUE ~ moe_prop/1.645/estimate_prop*100))
    #single variable
    } else { 
      test <- filter(acs_data_step1, variable %in% num) %>% 
        group_by(GEOID) %>% 
        summarise(estimate=sum(estimate), moe=moe_sum(moe, estimate)) %>% 
        mutate(cv = case_when(estimate == 0 ~ 100,
                              TRUE ~ moe/1.645/estimate*100))
    }
    
    cv_per <- round(nrow(filter(test, cv < 40))/nrow(test)*100)
    
    #is it valid?
    if (cv_per >= 60) {
      print(paste0(name, " is valid: ", cv_per, "%"))
    } else {
      print(paste0("ERROR, ", name, " is not valid: ", cv_per, "%. Do not include."))
    }
  }
  
  
  
  #check if each variable is valid
  check_validity("B19013_001", NA, "#median hh income") 
  #all hh above poverty
  check_validity("B17017_031", "B17017_001", "hh above poverty status")
  #family hh above poverty
  #check_validity("B17017_032", c("B17017_032", "B17017_003"), "family hh above poverty status")
  check_validity(c(paste0("B19001_00", c(2:9)), "B19001_010"), "B17017_001", "hh making less than 50k")
  check_validity(c(paste0("B01001_00", c(3:6)), paste0("B01001_0", c(27:30))), "B01001_001", "% under 18")
  check_validity(paste0("B01001_0", c(20:25, 44:49)), "B01001_001", "% over 65")
  check_validity(c(paste0("B01001_0", c(20:25, 44:49)),paste0("B01001_00", c(3:6)), paste0("B01001_0", c(27:30))), "B01001_001", "% under 18 or over 65")
  check_validity("B25003_002", "B25003_001", "% of residents who own their home")
  check_validity("B23025_007", "B23025_001", "% of unemployed residents")
  
  
  ###
  # also want to get regional averages for metadata
  ###
  acs_metadata <- get_acs(
    geography = "county",
    variables = c(acs_variables),
    survey = "acs5",
    state = state,
    county = county,
    year = acs_year
  ) %>%
    add_column(geo = "region averages") %>%
    group_by(geo, variable) %>%
    summarise(estimate = sum(estimate)) %>%
    pivot_wider(names_from = variable, values_from = estimate) %>%
     rename(
          hh_above_poverty = "B17017_031"
          #hh_above_poverty = "B17017_032"
        ) %>% 
    # process to useable forms of the data
    mutate(
      under18 = rowSums(across(c(
        paste0("B01001_00", c(3:6)), # under18 m
        paste0("B01001_0", c(27:30)) # under18 f
      )), na.rm = T),
      over65 = rowSums(across(c(
        paste0("B01001_0", c(20:25, 44:49)) # over 65m, f
      )), na.rm = T),
      hh_less_50k = rowSums(across(c(paste0("B19001_00", c(2:9)), "B19001_010")), na.rm = T),
    ) %>%
    mutate(across(c(under18, over65), ~ .x / B01001_001),
      #all hh above poverty
      across(c(hh_less_50k, hh_above_poverty), ~ .x / B17017_001),
      #family hh above poverty
      # hh_above_poverty = hh_above_poverty/(hh_above_poverty+B17017_003),
      # hh_less_50k = hh_less_50k/B17017_001,
      pownhome = B25003_002 / B25003_001,
      pwk_nowork = B23025_007 / B23025_001,
    ) %>%
    rename(hhincome = B19013_001) %>% 
    select(-starts_with("B"))
  
  #process the acs data  
  #transform the acs data  
  acs_data_step2 <- acs_data_step1 %>% 
    pivot_wider(names_from = variable, values_from = c(estimate, moe), names_vary = "slowest") %>%
    #calculate combined estimates and moes for some variables
    mutate(
      #hh below 50k
      estimate_hh_less_50k = rowSums(across(c(paste0("estimate_B19001_00", c(2:9)), "estimate_B19001_010")), na.rm = T),
      #under18
      estimate_under18 = rowSums(across(c(
        paste0("estimate_B01001_00", c(3:6)), # under18 m
        paste0("estimate_B01001_0", c(27:30)) # under18 f
      )), na.rm = T),
      #over 65
      estimate_over65 = rowSums(across(c(
        paste0("estimate_B01001_0", c(20:25, 44:49)) # over 65m, f
      )), na.rm = T),
    ) %>% 
    rowwise() %>% 
    mutate(
      #unfortunately the moe_sum function doesn't like vectors that have been pasted together, so typing them out manually
      #hh below 50k
      moe_hh_less_50k = moe_sum(moe=c(moe_B19001_002, moe_B19001_003, moe_B19001_004, moe_B19001_005, moe_B19001_006, moe_B19001_007, moe_B19001_008, moe_B19001_009, moe_B19001_010), estimate= c(estimate_B19001_002, estimate_B19001_003, estimate_B19001_004, estimate_B19001_005, estimate_B19001_006, estimate_B19001_007, estimate_B19001_008, estimate_B19001_009, estimate_B19001_010), na.rm = T),
      #under 18
      moe_under18 = moe_sum(moe = c(moe_B01001_003, moe_B01001_004, moe_B01001_005, moe_B01001_006, moe_B01001_027, moe_B01001_028, moe_B01001_029, moe_B01001_030), estimate = c(estimate_B01001_003, estimate_B01001_004, estimate_B01001_005, estimate_B01001_006, estimate_B01001_027, estimate_B01001_028, estimate_B01001_029, estimate_B01001_030), na.rm = T),
      #over 65
      moe_over65 = moe_sum(moe = c(moe_B01001_020, moe_B01001_021, moe_B01001_022, moe_B01001_023, moe_B01001_024, moe_B01001_025, moe_B01001_044, moe_B01001_045, moe_B01001_046, moe_B01001_047, moe_B01001_048, moe_B01001_049), estimate = c(estimate_B01001_020, estimate_B01001_021, estimate_B01001_022, estimate_B01001_023, estimate_B01001_024, estimate_B01001_025, estimate_B01001_044, estimate_B01001_045, estimate_B01001_046, estimate_B01001_047, estimate_B01001_048, estimate_B01001_049), na.rm = T)
    ) %>%
    #rename the columns
    rename(
      estimate_hhincome = "estimate_B19013_001",
      moe_hhincome = "moe_B19013_001",
      estimate_tenure_total = "estimate_B25003_001",
      moe_tenure_total = "moe_B25003_001",
      estimate_tenure_owners = "estimate_B25003_002",
      moe_tenure_owners = "moe_B25003_002",
      estimate_employment_denom = "estimate_B23025_001",
      moe_employment_denom = "moe_B23025_001",
      estimate_unemployed = "estimate_B23025_007",
      moe_unemployed = "moe_B23025_007",
      # estimate_hh_above_poverty = "estimate_B17017_032",
      # moe_hh_above_poverty = "moe_B17017_032",
      # estimate_hh_below_poverty = "estimate_B17017_003",
      # moe_hh_below_poverty = "moe_B17017_003",
      estimate_hh_above_poverty = "estimate_B17017_031",
      moe_hh_above_poverty = "moe_B17017_031",
      estimate_hhtotal = "estimate_B17017_001",
      moe_hhtotal = "moe_B17017_001",
      estimate_poptotal = "estimate_B01001_001",
      moe_poptotal = "moe_B01001_001"
    ) %>% 
    #get rid of columns used for sums
    select(-contains("B0"), -contains("B1")) %>% 
    #calculate estimate proportions
    mutate(
      #denom is population total
      across(c(estimate_under18, estimate_over65), ~ .x / estimate_poptotal * 100, .names = "prop_{.col}"),
      #denom is hh total - for all hh poverty
      across(c(estimate_hh_less_50k, estimate_hh_above_poverty), ~ .x / estimate_hhtotal * 100, .names = "prop_{.col}"),
      #for family hh poverty
      # prop_estimate_hh_less_50k = estimate_hh_less_50k / estimate_hhtotal * 100,
      # prop_estimate_hh_above_poverty = estimate_hh_above_poverty / (estimate_hh_above_poverty+estimate_hh_below_poverty) * 100,
      #denom is hh tenure
      prop_estimate_pownhome =  estimate_tenure_owners/ estimate_tenure_total * 100,
      #denom is employment
      prop_estimate_pwk_nowork = estimate_unemployed/ estimate_employment_denom * 100,
    ) %>%
    #calculate moe proportions
    mutate(
      prop_moe_under18 = moe_prop(estimate_under18, estimate_poptotal, moe_under18, moe_poptotal)*100,
      prop_moe_over65 = moe_prop(estimate_over65, estimate_poptotal, moe_over65, moe_poptotal)*100,
      prop_moe_hh_less_50k = moe_prop(estimate_hh_less_50k, estimate_hhtotal, moe_hh_less_50k, moe_hhtotal)*100,
      #for hh above poverty
      prop_moe_hh_above_poverty = moe_prop(estimate_hh_above_poverty, estimate_hhtotal, moe_hh_above_poverty, moe_hhtotal)*100,
      #for family hh above poverty
      #prop_moe_hh_above_poverty = moe_prop(estimate_hh_above_poverty, (estimate_hh_above_poverty+estimate_hh_below_poverty), moe_hh_above_poverty, (moe_hh_above_poverty+moe_hh_below_poverty))*100,
      prop_moe_pownhome = moe_prop(estimate_tenure_owners, estimate_tenure_total, moe_tenure_owners, moe_tenure_total)*100,
      prop_moe_pwk_nowork = moe_prop(estimate_unemployed, estimate_employment_denom, moe_unemployed, moe_employment_denom)*100,
    ) %>% 
    #drop raw estimate and MOE values
    select(GEOID, NAME, tract_id, estimate_hhincome, moe_hhincome, starts_with("prop")) %>% 
    pivot_longer(cols = 4:17, names_to = "variable", names_prefix = "prop_") %>% 
    #fix NAN values
    mutate(value = case_when(is.nan(value) ~ 0,
                             TRUE ~ value))

    estimates <- acs_data_step2 %>% 
      filter(grepl("estimate", variable)) %>% 
      mutate(variable = str_sub(variable, start = 10, end = -1L)) %>% 
      rename(estimate = value)
    
    moes <- acs_data_step2 %>% 
      filter(grepl("moe", variable)) %>% 
      mutate(variable = str_sub(variable, start = 5, end = -1L)) %>% 
      rename(moe = value)
    
    #the final acs dataset that will be used to cacluate significance
    acs_data <- left_join(estimates, moes, by = c("variable" = "variable", "GEOID" = "GEOID", "NAME" = "NAME", "tract_id" = "tract_id"))
  
  save(acs_data, acs_metadata, file = paste0(here::here(), "/data-raw/acs_data.rda"))
} else {
  load(paste0(here::here(), "/data-raw/acs_data.rda"))
}
```



## PLACES health data

Health metrics come from [PLACES: Local Data for Better Health, Census Tract Data](https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-Census-Tract-D/cwsq-ngmh). I am not aware that there is a way to use a "health_year" parameter to get a specific timestamp of the data, so please just double check that the most recent data is being used! 

```{r PLACES}
if (places_health_data == TRUE) {
  ##########
  # CDC health data
  #########
  # variable options are documented here: https://www.cdc.gov/places/measure-definitions/index.html
  
  raw_health <- read.socrata(
    # "https://chronicdata.cdc.gov/resource/cwsq-ngmh.json?$where=stateabbr in('MN', 'WI')",
    #paste0("https://chronicdata.cdc.gov/resource/cwsq-ngmh.json?$where=stateabbr in('", state, "')"),
    paste0(
      "https://chronicdata.cdc.gov/resource/cwsq-ngmh.json?stateabbr=",
      state,
      "&countyname=",
      county
    ),
    app_token = Sys.getenv("CDC_KEY"),
    email = Sys.getenv("CDC_EMAIL"),
    password = Sys.getenv("CDC_PASSWORD")
  ) %>%
    # get the columns of interest. there are more variables as an fyi though!
    # names(bg_health)
    # levels(as.factor(bg_health$measure))
    filter(
      measure %in% c(
        "Current asthma among adults aged >=18 years",
        "Chronic obstructive pulmonary disease among adults aged >=18 years",
        "Mental health not good for >=14 days among adults aged >=18 years",
        "Physical health not good for >=14 days among adults aged >=18 years"
      )
    ) %>%
    transform(
      data_value = as.double(data_value),
      low_confidence_limit = as.double(low_confidence_limit),
      high_confidence_limit = as.double(high_confidence_limit)
    ) %>%
    mutate(moe = (high_confidence_limit - low_confidence_limit) / 2) %>%
    rename(estimate = data_value) %>%
    dplyr::select(locationname, measureid, estimate, moe) %>%
    mutate(estimate = estimate / 100) # change to fraction

  crosswalk <-
    read_csv(
      paste0(
        here::here(),
        "/data-raw/nhgis_bg2020_tr2010_55.csv"
      ),
      col_types = c("tr2010ge" = "c",
                    "bg2020ge" = "c")
    ) %>%
    select(tr2010ge, # census geoid for 2010
           bg2020ge, # census geoid for 2020
           wt_pop) %>% # use weighted population crosswalk
    group_by(bg2020ge) %>%
    slice(which.max(wt_pop)) %>% # just get the crosswalk for where most people live. it's oversimplified, but pretty decent esp since block groups generally nest nicely into tracts across census geographies
    select(-wt_pop)
  
  ## translate health data into 2020 geographies
  health_data <- raw_health %>%
    full_join(crosswalk, by = c("locationname" = "tr2010ge")) %>%
    rename(GEOID = bg2020ge) %>%
    select(-locationname) %>% 
    na.omit()
  
  ## check that the variables are valid, since they have a moe
  check_validity <- function(num, name) {
    test <- filter(health_data, measureid %in% num) %>%
      group_by(GEOID) %>%
      summarise(estimate = estimate * 100, moe = moe_sum(moe, estimate)) %>%
      mutate(cv = case_when(estimate == 0 ~ 100,
                            TRUE ~ moe / 1.645 / estimate * 100))
    
    cv_per <- round(nrow(filter(test, cv < 40)) / nrow(test) * 100)
    
    #is it valid?
    if (cv_per >= 60) {
      print(paste0(name, " is valid: ", cv_per, "%"))
    } else {
      print(paste0("ERROR, ", name, " is not valid: ", cv_per, "%. Do not include."))
    }
  }
  
  
  check_validity("PHLTH", "Current asthma among adults aged >=18 years")
  check_validity("COPD",
                 "Chronic obstructive pulmonary disease among adults aged >=18 years")
  check_validity("PHLTH",
                 "Physical health not good for >=14 days among adults aged >=18 years")
  check_validity("MHLTH",
                 "Mental health not good for >=14 days among adults aged >=18 years")
  
  save(health_data, file = paste0(here::here(), "/data-raw/health_data.rda"))
} else {
  load(paste0(here::here(), "/data-raw/health_data.rda"))
}
```


#ejscreen

TODO - get rid of ejscreen?

Although the original 01_tutorial code had this as an automated download, I was unable to get it to work. I suggest downloading manually from here: https://gaftp.epa.gov/EJSCREEN/. There will likely be several downloads available - you want the one with 'tracts' in the name.

The names look different now then when I downloaded... also I was using 2023 info, not sure if I should've since we were partway through the year?

```{r ejscreen}

if (ejscreen == TRUE) {
  #uncomment below to try and download automatically
  # temp <- tempfile()
  # temp2 <- tempfile()
  # download.file(
  #   #"https://gaftp.epa.gov/EJSCREEN/2021/EJSCREEN_2021_USPR_Tracts.csv.zip",
  #   "https://gaftp.epa.gov/EJSCREEN/2023/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI.csv.zip",
  #   destfile = temp
  # )
  # unzip(zipfile = temp, exdir = temp2)
  # list.files(temp2)

  #ejscreen <- read_csv(paste0(temp2, pattern = "/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI.csv"),
  ejscreen <- read_csv("data-raw/EJSCREEN_2023_Tracts_with_AS_CNMI_GU_VI.csv",
    col_select = c(ST_ABBREV, CANCER, ID)
  ) %>%
    filter(ST_ABBREV == state)

  # #ejscreen is still using 2010 geographies
  # ejscreen %>% left_join(bg_geo %>%
  #                          mutate(ID = substr(GEOID, start = 1, stop = 11))) %>%
  #   st_as_sf() %>%
  #   ggplot() +
  #   geom_sf(aes(fill = CANCER))

  files <- list.files(temp2, full.names = T)
  file.remove(files)

  ## translate into 2020 geographies
  cancer_data <- ejscreen %>%
    full_join(crosswalk, by = c("ID" = "tr2010ge")) %>%
    rename(bg_id = bg2020ge) %>%
    select(bg_id, CANCER) %>%
    mutate(CANCER = if_else(is.na(CANCER), min(CANCER, na.rm = T), CANCER)) %>% # don't let there be NAs, fill with the minimum value
    # filter(!is.na(bg_id)) %>%
    rename(env_cancer = CANCER)

  save(file = paste0(here::here(), "/data-raw/cancer_data.rda"), cancer_data)
} else {
  load(paste0(here::here(), "/data-raw/cancer_data.rda"))
}

```

#block groups

Should this only happen if the decennial census is new? Otherwise, just use the already downloaded copy I think... then save over if not.
Also redo the historical stuff & temperature here if decennial_census = TRUE?

```{r blockgroups}
if (decennial_census == TRUE) {
  bg_geo <- block_groups(
    state = state,
    county = county,
    year = year
  ) %>% 
    remove_lake_bgs("GEOID")
  
  
  st_write(ctu_geo, paste0(here::here(), "/data-raw/gee_canopy_data/blockgroups/bg.shp"), append = FALSE)
}else{
  bg_geo <- read_sf(paste0(here::here(), "/data-raw/gee_canopy_data/blockgroups/bg.shp"))
}
```

# Cities, Towns, Unincorporated

Automated download and processing of the 'Municipal Boundaries' shapefile. Can also be manually downloaded here: https://gis-countyofdane.opendata.arcgis.com/pages/boundaries

```{r ctu}
if (cities_towns_unincorporated == TRUE) {

  ctu_geo <-
    #download from the DC rest endpoint
    get_spatial_layer(
      "https://dcimapapps.countyofdane.com/arcgissrv/rest/services/Administrative/WardBoundaries/MapServer/0"
    ) %>%
    #merge geometries with same name
    group_by(NAME) %>% 
    summarize(geometry = st_union(geoms)) %>%
    #simplify names
    mutate(short = strsplit(NAME, " of ")) %>%
    unnest_wider(short, names_sep = "_") %>%
    group_by(short_2) %>%
    mutate(n = n()) %>%
    #specify the type of municipality for any duplicate names
    mutate(NAME = case_when(n > 1 ~ paste0(short_2, " (", short_1, ")"),
                            TRUE ~ short_2)) %>%
    ungroup() %>%
    arrange(NAME) %>%
    rename(GEO_NAME = NAME) %>%
    select(GEO_NAME, geometry) %>%
    #convert back to a sf object
    st_as_sf("/data-raw/gee_canopy_data/cities/ctu.shp")
  
  #TODO the projection info is different than what I originally saved, is this a problem?
  #TODO should save over ctu, saving seperate file right now for testing
  #save over previous ctu shapefile 
  st_write(ctu_geo, paste0(here::here(), "/data-raw/gee_canopy_data/cities/ctu2.shp"), append = FALSE)
} else {
  ctu_geo <- read_sf(paste0(here::here(), "/data-raw/gee_canopy_data/cities/ctu.shp"))
}

```


# Neighborhoods

TODO - Matt had done some work in GIS to clean up overlapping polygons -- need to figure out how to do that programatically and add in

```{r nhood}

if (neighborhoods == TRUE) {
  
  nhood_geo <-
    #download from the Madison rest endpoint
    get_spatial_layer(
      "https://maps.cityofmadison.com/arcgis/rest/services/Public/OPEN_DATA/MapServer/12/"
    ) %>%
    rename(GEO_NAME = NEIGHB_NAME, geometry = geoms) %>%
    filter(STATUS == "Active") %>%
    #trim whitespace
    mutate(GEO_NAME = str_trim(GEO_NAME)) %>% 
    #delete some overalapping neighborhoods within larger ones
    filter(!(GEO_NAME %in% c("Parkwood Village Homeowners", "Parkwood West Condominium", "Allied Dunn's Marsh", "Bayview Foundation Inc.", "Brittingham Apartments Resident's", "Schenk-Atwood Revitalization"))) %>% 
    mutate(city = "Madison") %>%
    select(GEO_NAME, city, geometry) %>% 
    st_transform(st_crs(ctu_geo))
    

  
  # tst <- read_sf(paste0(here::here(), "/data-raw/gee_canopy_data/neighborhoods/nhoods-clean-2023/nhoods.shp")) %>% 
  #   mutate(city = "Madison") #%>%
  #   select(GEO_NAME, city, geometry)
  # 
  #  
  # #shorten the neighborhood names by removing suffixes that are not necessary for our purposes
  # shorten_name <- function(x) { 
  #   find_i <- gregexpr('Neighborhood Association|Condominium Association|Condominum Association|Condominium|Homeowners Association|Community Association|Home Owners Association|Homeowners Assoc|Neighborhood As|Owners Assoc|Owners Association|Association', x)[[1]][1]
  #   if (find_i > 0) {
  #     return (substr(x, 1, find_i-2))
  #   } else {
  #     return(x)
  #   }
  # }
  
  #nhood_geo$GEO_NAME <- sapply(nhood_geo$GEO_NAME, shorten_name, USE.NAMES = FALSE)
  
  #save over previous nhood shapefile 
  st_write(nhood_geo, paste0(here::here(), "/data-raw/gee_canopy_data/neighborhoods/nhood.shp"), append = FALSE)
  
  gs <- st_drop_geometry(nhood_geo) %>%
  select(city, GEO_NAME) %>%
    arrange(GEO_NAME) %>% 
  group_by(city)
nhood_ui <- group_split(gs, city, .keep = F) %>% purrr::flatten() # set_names(group_keys(gs))
names(nhood_ui) <- c(group_keys(gs))[[1]]

save(file = paste0(here::here(), "/data/nhood_ui.rda"), nhood_ui)
usethis::use_data(nhood_ui, overwrite = TRUE)

} else {
  nhood_geo <- read_sf(paste0(here::here(), "/data-raw/gee_canopy_data/neighborhoods/nhood.shp"))
}

```

# Crosswalk

```{r ctu-nhood-crosswalk}

if (decennial_census == TRUE | cities_towns_unincorporated == TRUE | neighborhoods == TRUE) {
# fxns to make easy -----
# find crosswalks
find_crosswalks <- function(x, buffer) {
  crosswalk <- x %>%
  st_transform(26915) %>%
  #buffer to not catch small edges in the crosswalk
  st_buffer(buffer) %>% 
  st_intersection(bg_geo %>% 
                    dplyr::select(GEOID) %>%
                    rename(bg_id = GEOID) %>%
                    st_transform(26915)) %>%
  st_drop_geometry() %>% 
  #filter out the two block groups that make up lake mendota and monona
  remove_lake_bgs("bg_id")

  return(crosswalk)
}

ctu_crosswalk <- find_crosswalks(ctu_geo, -150)

nhood_crosswalk <- find_crosswalks(nhood_geo, 50) %>% 
#one nhood is too small to survive the buffer so add it back in manually
  add_row(GEO_NAME = "Wexford Village Condominium Owners Assoc", city = "Madison", bg_id = "550250002041")

wide_ctu_crosswalk <- ctu_crosswalk %>%
    aggregate(GEO_NAME ~ bg_id, paste, collapse = ", ") %>%
  rename(jurisdiction = GEO_NAME)

  save(ctu_crosswalk, nhood_crosswalk, wide_ctu_crosswalk, file = paste0(here::here(), "/data-raw/geography_data.rda"))
} else {
  load(paste0(here::here(), "/data-raw/geography_data.rda"))
}

```


#Temperature

```{r temperature}
if (temperature == TRUE) {
  # Block group geometries are downloaded in 01_tutorial.Rmd
#TODO - am I including lake areas in the temperature?
bg <- vect(arrange(bg_geo, GEOID))
#the average land surface temperature
avgTemp <- rast(paste0(here::here(), "/data-raw/LST_2021_summer_max.tif"))
#convert temperatures from Kelvin to Fahrenheit
avgTemp <- project(avgTemp, bg)
avgTemp$tempF <- (avgTemp$LST - 273.15) * 9/5 + 32

lui <- vect(paste0(here::here(), "/data-raw/gee_canopy_data/water/LUI_2020_Dane_County_Water.shp"))
lui <- project(lui, bg)
lakes <- lui[grepl('Lake', lui$PLACE_DESC)]
#lake mendota and monona area their own block groups and already removed, so don't need to include them here
lakes <- lakes[lakes$PLACE_DESC != "Lake Mendota" & lakes$PLACE_DESC != "Lake Monona"]

#mask out water - the low resolution makes this not perfect by any means, but we're averaging within block groups so I think this is ok
avgTemp <- mask(avgTemp, lakes, inverse = TRUE, touches = FALSE)
#calculate the mean temperature within each block group
bg_temp <- zonal(avgTemp$tempF, bg, "mean", na.rm=TRUE)

clim_vul <- data.frame(matrix(nrow = 351, ncol = 2))
columns= c("BG55", "AVG_TEMP")
colnames(clim_vul) = columns

clim_vul$BG55 <- bg$GEOID
clim_vul$AVG_TEMP <- bg_temp$tempF

#write csv
write.csv(clim_vul, paste0(here::here(), "/data-raw/CLIMATE_BG55.csv"), row.names = FALSE)
} else {
  clim_vul <- read.csv(paste0(here::here(), "/data-raw/CLIMATE_BG55.csv"))
}
```


# Canopy

The rest of the update relies on canopy information derived using Google Earth Engine, so you'll need to do a few things outside of this script.

1. If you don't already have access to GEE, use these instructions: https://developers.google.com/earth-engine/guides/access#individual-signup

2. Go to this script: https://code.earthengine.google.com/d34583e1d03f11b2c02ec7a3a0e2592d

3. You will need to replace imported assets with versions you have uploaded locally. The files are located in individual folders in data-raw/gee_canopy_data and share the import variable names. For more details on importing assets see https://developers.google.com/earth-engine/guides/asset_manager.

TODO - again, can I just share these with the IT account somehow?


4. Follow the instructions at the top of the script. You should run the script 3 times and download a data set for each geography. Save the resulting datasets in the data-raw folder.

5. In the code below, change the filepaths to refer to the new data.



```{r canopy}
if (canopy == TRUE) {
  
# The files where the canopy data is stored - change these when updating to the names of the files downloaded from GEE
  bg_canopy <- read_csv(paste0(here::here(), "/data-raw/DC_TreeAcres_blockgroups_year2023.csv")) %>% 
  arrange(GEOID) %>% 
  remove_lake_bgs("GEOID")

  ctu_list_raw <- read_csv(paste0(here::here(), "/data-raw/DC_TreeAcres_ctu_year2023.csv")) %>% 
  arrange(GEO_NAME)
  
  nhood_list <- read_csv(paste0(here::here(), "/data-raw/DC_TreeAcres_nhood_year2023.csv")) %>% 
  arrange(GEO_NAME)
  
#Process for block groups

#just using the same coefficient for the twin cities for now, will potentially calculate for Madison later
calib_coeff <- .88


bg_geo <- arrange(bg_geo, GEOID)
bg_canopy$ALAND <- bg_geo$ALAND

bg <- vect(bg_geo)
#calculate total area of all block groups in meters
total_area <- expanse(bg,"m")

bg_canopy <- bg_canopy %>% 
  rename('trees' = '1', 'treeless' = '0') %>% 
  #convert square acres (GEE output) to meters
  mutate(trees = trees*4046.856422,
         treeless = total_area - trees,
         treeless = if_else(ALAND == 0, 0, as.numeric(treeless)),
         trees = if_else(ALAND == 0, 0, as.numeric(trees)),
         canopy_percent = trees / (trees + treeless) * calib_coeff, 
         canopy_percent = if_else(is.na(canopy_percent), 0, canopy_percent)) %>%
    filter(ALAND != 0) %>% #new 2020 block groups sometimes are only water
    mutate(avgcanopy = mean(canopy_percent)) %>%
    select(-ALAND, -trees, -treeless) %>% 
    rename(bg_id = GEOID) %>%
    transform(bg_id = as.character(bg_id))
   


#process for ctu

ctu_geo <- st_read(paste0(here::here(), "/data-raw/gee_canopy_data/cities/ctu.shp")) %>% 
  arrange(GEO_NAME)

ctu <- vect(ctu_geo)
#calculate total area of all cities in meters
total_area <- expanse(ctu,"m")

  

ctu_list_raw <- ctu_list_raw %>% 
  rename('trees' = '1', 'treeless' = '0') %>% 
  #convert square acres (GEE output) to meters
  mutate(trees = trees*4046.856422,
         treeless = total_area - trees,
         canopy_percent = trees / (trees + treeless) * calib_coeff, 
         canopy_percent = if_else(is.na(canopy_percent), 0, canopy_percent)) %>%
    mutate(avgcanopy = mean(canopy_percent)) %>%
    select(-trees, -treeless) %>%
  full_join(left_join(ctu_crosswalk, bg_canopy) %>% 
  group_by(GEO_NAME) %>%
  summarise(
    min = round(min(canopy_percent) * 100, 1),
    max = round(max(canopy_percent) * 100, 1),
    n_blockgroups = n()
  )) %>%
  arrange(GEO_NAME) %>%
  full_join(ctu_geo) %>%
  st_as_sf()


#neighborhoods

nhood_geo <- st_read(paste0(here::here(), "/data-raw/gee_canopy_data/neighborhoods/nhood.shp")) %>% 
  arrange(GEO_NAME)

nhood <- vect(nhood_geo)
#calculate total area of all cities in meters
total_area <- expanse(nhood,"m")

nhood_list_raw <- nhood_list %>% 
  rename('trees' = '1', 'treeless' = '0') %>% 
  #convert square acres (GEE output) to meters
  mutate(trees = trees*4046.856422,
         treeless = total_area - trees,
         canopy_percent = trees / (trees + treeless) * calib_coeff, 
         canopy_percent = if_else(is.na(canopy_percent), 0, canopy_percent)) %>%
    mutate(avgcanopy = mean(canopy_percent)) %>%
    select(-trees, -treeless) %>% 
  full_join(left_join(nhood_crosswalk, bg_canopy) %>% 
  group_by(GEO_NAME, city) %>%
  summarise(
    min = round(min(canopy_percent) * 100, 1),
    max = round(max(canopy_percent) * 100, 1),
    n_blockgroups = n()
  )) %>%
  full_join(nhood_geo) %>%
  st_as_sf()

  save(bg_canopy, ctu_list_raw, nhood_list_raw, file = paste0(here::here(), "/data-raw/canopy_data.rda"))
  
} else {
  load(paste0(here::here(), "/data-raw/canopy_data.rda"))
}
```


write instructions... don't feel like it right now...

#holc data


```{r redlining}

#download historical shapefile from https://github.com/americanpanorama/Census_HOLC_Research/tree/main/2020_Census_Tracts, then put in the data-raw folder

#limit to Dane County and redlined areas (grade D)
#TODO cut out water/non residential areas like they did in original code? Not much is included, so I don't know if this is necessary
redline_raw <- st_read("data-raw/holc_census_tracts/holc_census_tracts.shp") %>% 
  filter(state == "WI" & county_cod == "025" & holc_grade == "D") %>% 
  sf::st_transform(4326) %>% 
  remove_lake_bgs("geoid")

bg <- vect(bg_geo)

holc <- vect(redline_raw) %>% 
  project(bg)

holc_data <- data.frame(matrix(nrow = 351, ncol = 2))
columns= c("bg_id", "holc_pred")
colnames(holc_data) = columns

holc_data$bg_id <- bg$GEOID

#calculate the percentage of redlined area within each block group
for (i in 1:nrow(bg)) {
  clip <- crop(holc, bg[i,])
  perc <- round(sum(expanse(clip))/expanse(bg[i,]), digits = 3)
  if (length(perc > 0)) {
    holc_data[i,2] <- perc
  } else {
    holc_data[i,2] <- 0
  }
}

write.csv(holc_data, paste0(here::here(), "/data-raw/holc2.csv"), row.names = FALSE)
lui <- vect(paste0(here::here(), "/data-raw/LUI_2020_Dane_County/LUI_2020_Dane_County_Project.shp"))
lui <- project(lui, bg)
tst <- crop(lui, holc)
a <- tst[tst$GENERALIZE == "RESIDENTIAL"]
```

# Final processing

Stuff to get the final versions in the data folder

```{r}
 county_outline <- if (is.null(county)) {
    tigris::counties(state = state)
  } else {
    tigris::counties(state = state) %>%
      filter(NAME %in% county)
  }
```


NEW processing

```{r}
#Process variables with and without MOE separately

#80th percentile without MOE

#only the race related variables have the possibility of scoring an extra point
no_extra_point <- c("housing_density", "pop_density")

#Decennial variables
decennial_sig <- decennial_data %>%  
  left_join(bg_geo %>% select(GEOID, ALAND), by = c("GEOID" = "GEOID")) %>% 
  #calculate housing density
  mutate(housing_density = housing_units/ALAND*4045.86, .before = fancyname) %>% 
  mutate(pop_density = population_count/ALAND*4045.86, .before = fancyname) %>% 
  select(-housing_units, -population_count, -ALAND, -whitenh) %>% 
  pivot_longer(2:9, names_to = "variable") %>% 
  #calculate percentage populations
  group_by(variable) %>% 
  #calculate the thresholds for 80th and 95th percentiles
  mutate(thresh_80 = quantile(value, probs=.8, na.rm=T)) %>% 
  mutate(thresh_95 = quantile(value, probs=.95, na.rm=T)) %>% 
  mutate(sig_80 = case_when(value > thresh_80 ~ 1,
                            TRUE ~ 0)) %>% 
  mutate(sig_95 = case_when(value > thresh_95 & !(variable %in% no_extra_point) ~ 1,
                            TRUE ~ 0)) %>% 
  mutate(significance = sig_80 + sig_95) %>% 
  rename(raw_value = value) %>% 
  select(variable, GEOID, significance, raw_value) 

#temperature
clim_vul_sig <- clim_vul %>% 
  #calculate the thresholds for 80th percentile
  mutate(thresh = quantile(AVG_TEMP, probs=.8, na.rm=T)) %>% 
  mutate(significance = case_when(AVG_TEMP > thresh ~ 1,
                            TRUE ~ 0)) %>% 
  rename(raw_value = AVG_TEMP, GEOID = BG55) %>% 
  mutate(variable = "avg_temp") %>% 
  select(variable, GEOID, significance, raw_value) %>% 
  mutate(GEOID = as.character(GEOID))

#canopy
bg_canopy_sig <- bg_canopy %>% 
  mutate(canopy_percent = canopy_percent * 100) %>% 
  #calculate the thresholds for 20th percentile, since we're interested in areas with low canopy
  mutate(thresh = quantile(canopy_percent, probs=.2, na.rm=T)) %>% 
  mutate(significance = case_when(canopy_percent < thresh ~ 1,
                            TRUE ~ 0)) %>% 
  mutate(variable = "canopy_percent") %>% 
  rename(raw_value = canopy_percent, GEOID = bg_id) %>% 
  select(variable, GEOID, significance, raw_value) 
  
#significance testing with MOE

#ACS variables


ACS_Value_Update <- function(Estimate, MOE, high_low){

  values_df <- data.frame(Estimate, MOE)

  median_idx <- length(Estimate) / 2

  if(round(median_idx) != median_idx)

    median_idx <- median_idx + 0.5

  median_estimate <- sort(Estimate)[median_idx]

  median_MOE <- MOE[which(Estimate == median_estimate)[1]]
  print(median_estimate)
  print(median_MOE)


  z_score <- abs(median_estimate - Estimate)/sqrt((median_MOE^2)+(MOE^2))

  

  value <- ifelse(z_score > 1 & Estimate > median_estimate, "HIGH",

                  ifelse(z_score > 1 & Estimate < median_estimate, "LOW", "MEDIUM"))

  value <- ifelse(value == high_low, 1, 0)
  
  return(z_score)

}


low_op_vars <- c("hhincome", "hh_above_poverty", "pownhome")

#z-scores for variables with low opportunity
acs_sig_low <- acs_data %>%
  filter(variable %in% low_op_vars) %>% 
  group_by(variable) %>% 
  #mutate(across(c(estimate, moe), fix_nan)) %>% 
  mutate(significance = ACS_Value_Update(estimate, moe, "LOW"))

#z-scores for variables with high opportunity
acs_sig_high <- acs_data %>%
  filter(!(variable %in% low_op_vars)) %>% 
  group_by(variable) %>% 
  #mutate(across(c(estimate, moe), fix_nan)) %>% 
  mutate(significance = ACS_Value_Update(estimate, moe, "HIGH"))

acs_sig <- bind_rows(acs_sig_low, acs_sig_high) %>% 
  rename(raw_value = estimate) %>%
  select(variable, GEOID, significance, raw_value) %>% 
  #for missing data, say those bgs are not significant
  mutate(significance = case_when(is.na(significance) ~ 0,
                                  TRUE ~ significance))


#PLACES
health_data_sig <- health_data %>% 
  mutate(estimate = estimate * 100) %>% 
  group_by(measureid) %>% 
  mutate(significance = ACS_Value_Update(estimate, moe, "HIGH")) %>% 
  rename(variable = measureid, raw_value = estimate) %>% 
  select(variable, GEOID, significance, raw_value)

#join everything together

code_metadata <- read_csv(paste0(here::here(), "/data-raw/metadata_2.csv"))

bg_growingshade_main <- bind_rows(decennial_sig, clim_vul_sig, bg_canopy_sig, acs_sig, health_data_sig) %>% 
  rename(bg_string = GEOID) %>% 
  #add in fancy names
  left_join(code_metadata, by = c("variable" = "variable"))

demo_metadata <- acs_metadata %>%
  full_join(decennial_metadata) %>%
  pivot_longer(names_to = "variable", values_to = "MEANRAW2", -geo) %>% 
  mutate(MEANRAW2 = case_when(!(variable %in% c("hhincome", "housing_units", "population_count")) ~  MEANRAW2 * 100,
                              TRUE ~ MEANRAW2))

bg_averages <- bg_growingshade_main %>%
  group_by(variable) %>%
  summarise(
    MEANRAW = mean(raw_value, na.rm = T),
    #toggle for binary variables
    #MEANSCALED = mean(weights_scaled, na.rm = T)
  )

canopy_avg <-
  tribble(
    ~variable, ~MEANRAW2,
    "canopy_percent", ctu_list_raw$avgcanopy[1],
    "inverse_canopy", ctu_list_raw$avgcanopy[1]
  ) %>%  
  mutate(MEANRAW2 = MEANRAW2 * 100)

  


metadata <- bg_growingshade_main %>%
  dplyr::group_by(type, name, variable, interpret_high_value, temperature, socioeconomic_indicators, health_disparities, canopy_cover) %>%
  dplyr::count() %>%
  dplyr::ungroup() %>%
  full_join(bg_averages) %>%
  mutate(
    niceinterp =
      case_when(
        interpret_high_value == "high_opportunity" ~ "Higher",
        TRUE ~ "Lower"
      ),
    nicer_interp = case_when(
      niceinterp == "Lower" ~ "Lower values = higher priority",
      TRUE ~ ""
    )
  ) %>%
  full_join(demo_metadata %>%
    bind_rows(canopy_avg)) %>%
  mutate(MEANRAW = if_else(!is.na(MEANRAW2), MEANRAW2, MEANRAW)) %>%
  dplyr::select(-MEANRAW2, -geo) %>%
  filter(!is.na(name)) %>% 
  mutate(custom = 0, .before = n)

usethis::use_data(metadata, overwrite = TRUE)

highest_p <- function(group_var) {
  selectedvars <- metadata %>%
    filter(!!enquo(group_var) == 1) %>%
    .[, 2]
  
    bg_growingshade_main %>%
    filter(name %in% selectedvars$name) %>%
    group_by(bg_string) %>%
    summarise(MEAN = sum(significance, na.rm = F))
}

#need to process socieoconomic variables differently to cap race var score at 3
    selectedvars <- metadata %>%
    filter(socioeconomic_indicators == 1) 
      
    race_vars <- filter(selectedvars, grepl("Race", name))
    acs_vars <- filter(selectedvars, !grepl("Race", name))
    
    highest_p_socioec_step1 <- bg_growingshade_main %>%
    filter(name %in% race_vars$name) %>%
    group_by(bg_string) %>%
    summarise(MEAN = sum(significance, na.rm = F)) %>% 
    mutate(MEAN = case_when(MEAN > 3 ~ 3,
                            TRUE ~ MEAN))
    
    highest_p_socioec <- bg_growingshade_main %>%
    filter(name %in% acs_vars$name) %>%
    group_by(bg_string) %>%
    summarise(MEAN = sum(significance, na.rm = F)) %>%
    #add in capped race var scores
    bind_rows(highest_p_socioec_step1) %>%
    #resummarise
    group_by(bg_string) %>%
    summarise(MEAN = sum(MEAN, na.rm = F))
    

priority_summary_1 <- highest_p(health_disparities) %>%
  rename(`Health Disparities` = MEAN) %>%
  full_join(highest_p(temperature) %>% rename(Temperature = MEAN)) %>%
  full_join(highest_p_socioec %>% rename(`Socioeconomic Indicators` = MEAN)) %>%
  full_join(highest_p(canopy_cover) %>% rename(`Canopy Cover` = MEAN)) %>%
  pivot_longer(names_to = "preset", values_to = "score", -bg_string)

priority_summary <- priority_summary_1 %>%
  group_by(bg_string) %>%
  summarise(score = max(score, na.rm = T)) %>%
  left_join(priority_summary_1) %>%
  rename(highest_priority = preset) %>%
  rename(GEOID = bg_string)

mn_bgs_raw <- bg_geo %>%
  right_join(decennial_data %>% select(GEOID, fancyname)) %>%
  right_join(wide_ctu_crosswalk, by = c("GEOID" = "bg_id")) %>%
  full_join(bg_canopy, by = c("GEOID" = "bg_id")) %>%
  #full_join(priority_summary) %>%
  full_join(priority_summary_1 %>%
    group_by(preset) %>%
    pivot_wider(names_from = preset, values_from = score),
  by = c("GEOID" = "bg_string")
  ) %>%
  mutate(avgcanopy = mean(canopy_percent, na.rm = T)) %>%
  dplyr::select(-c(STATEFP, COUNTYFP, TRACTCE, BLKGRPCE, NAMELSAD, MTFCC, FUNCSTAT, INTPTLAT, INTPTLON)) %>%
  sf::st_as_sf() %>%
  sf::st_transform(4326) %>%
  #filter(!is.na(highest_priority)) %>%
  rename(GEO_NAME = GEOID) %>% 
  mutate(Custom = 0) %>% 
  #TODO probably makes more sense to move to canopy section
  mutate(canopy_percent = canopy_percent * 100,
         avgcanopy = avgcanopy * 100)
 

region_outline <- if (is.null(county)) {
  county_outline %>%
    group_by(STATEFP) %>%
    summarise(geometry = sf::st_union(geometry)) %>%
    sf::st_simplify(dTolerance = 400) %>%
    sf::st_transform(4326)
} else {
  county_outline %>%
    group_by(COUNTYFP) %>%
    summarise(geometry = sf::st_union(geometry)) %>%
    sf::st_simplify(dTolerance = 400) %>%
    sf::st_transform(4326)
}
usethis::use_data(region_outline, overwrite = TRUE)


speed_up <- function(x, smooth) {
  x %>%
    sf::st_transform(26915) %>%
    sf::st_simplify(dTolerance = smooth, preserveTopology = T) %>%
    sf::st_transform(4326)
}
# nhood_list <- nhood_list %>% st_make_valid() %>% st_simplify(dTolerance = 100) %>% st_as_sf()
ctu_list <- ctu_list_raw %>% 
    #TODO probably makes more sense to move to canopy section
  mutate(canopy_percent = canopy_percent*100,
         avgcanopy = avgcanopy*100) %>% 
  speed_up(50)
usethis::use_data(ctu_list, overwrite = TRUE)

nhood_list <- nhood_list_raw %>%
    #TODO probably makes more sense to move to canopy section
  mutate(canopy_percent = canopy_percent*100,
         avgcanopy = avgcanopy*100) %>% 
  speed_up(50)
usethis::use_data(nhood_list, overwrite = TRUE)

redline <- redline_raw %>%
  speed_up(50)
usethis::use_data(redline, overwrite = TRUE)

mn_bgs <- mn_bgs_raw %>%
  speed_up(40) %>% # 25
  filter(!is.na(GEO_NAME)) %>%
  mutate(GEOID = GEO_NAME)

usethis::use_data(mn_bgs, overwrite = TRUE)

# object.size(mn_bgs_raw) / 1e5 ; object.size(mn_bgs) / 1e5

bg_growingshade_main <- bg_growingshade_main %>%
  dplyr::select(bg_string, name, significance, raw_value) %>%
  filter(!is.na(bg_string))
usethis::use_data(bg_growingshade_main, overwrite = TRUE)

```

## Set user interface parameters

Creating a `ui_params.rda` data set is the easiest solutions that I can (currently!) think of which allows for site-specific parameters to be set *outside* of the main application code. For instance, other regions are unlikely to have a city called "Oakdale," but regardless a starting city needs to be identified in order for the map to render correctly. 

```{r}
map_centroid <- region_outline %>%
  st_union() %>%
  st_centroid()

ui_params <- tribble(
  ~param, ~set, ~number,
  "cityselected", if (nrow(ctu_list) >= 60) {
    ctu_list[[27, 1]]
  } else {
    ctu_list %>%
      arrange(-n_blockgroups) %>%
      .[[1, 1]]
  }, NA_real_, #"Madison", NA_real_,
  "nhoodselected", "Appalachian Ridge", NA_real_,
  "center_latitude", NA_character_, st_coordinates(map_centroid)[2], # 44.963,
  "center_longitude", NA_character_, st_coordinates(map_centroid)[1], #-93.32,
  "center_zoom", NA_character_, 10,
  "tree_tile_location", "https://metropolitan-council.github.io/treeraster-2021/GrowingShadeTealTrees_2021_toCloud/{z}/{x}/{y}", NA_real_
)

usethis::use_data(ui_params, overwrite = TRUE)

# nhood_ui <- list(Minneapolis = nhood_list$GEO_NAME[nhood_list$city == "Minneapolis"],
#           `St. Paul` = nhood_list$GEO_NAME[nhood_list$city == "St. Paul"])
gs <- st_drop_geometry(nhood_list) %>%
  select(city, GEO_NAME) %>%
  group_by(city)
nhood_ui <- group_split(gs, city, keep = F) %>% purrr::flatten() # set_names(group_keys(gs))
names(nhood_ui) <- c(group_keys(gs))[[1]]

usethis::use_data(nhood_ui, overwrite = TRUE)
```


# Update app

The code to test, then update the app


Try combining everything and mapping
```{r}

dec_total <- decennial_sig %>% 
  filter(variable %in% c("pasiannh", "pblacknh", "phisppop")) %>% 
  group_by(GEOID) %>% 
  summarise(total = sum(significance)) %>% 
  mutate(total = case_when(total > 3 ~ 3,
                           TRUE ~ total))

acs_total <- acs_sig %>% 
  filter(variable %in% c("hhincome", "hh_above_poverty", "hh_less_50k")) %>% 
  group_by(GEOID) %>% 
  summarise(total = sum(significance))

eq_total <- bg_growingshade_main_2 %>% 
  filter(variable %in% c("hhincome", "hh_above_poverty", "hh_less_50k", "pasiannh", "pblacknh", "phisppop")) %>% 
  group_by(bg_string) %>% 
  summarise(score = sum(significance))


temp_total <- bg_growingshade_main_2 %>% 
  filter(variable == "avg_temp") %>% 
  group_by(bg_string) %>% 
  summarise(score = sum(significance))

canopy_total <- bg_growingshade_main_2 %>% 
  filter(variable == "canopy_percent") %>% 
  group_by(bg_string) %>% 
  summarise(score = sum(significance))

health_total <- bg_growingshade_main_2 %>% 
  filter(variable %in% c("MHLTH", "PHLTH", "COPD", "CASTHMA")) %>% 
  group_by(bg_string) %>% 
  summarise(score = sum(significance))

#social indicators theme
map_2020 <- dec_total %>%
  left_join(acs_total) %>% 
  group_by(GEOID) %>% 
  summarise(score = sum(total)) %>% 
  filter(score >= 2) %>% 
  left_join(bg_geo, by = join_by(GEOID == GEOID))

#temperature theme
map_2020 <- temp_total %>%
  filter(score >= 1) %>% 
  left_join(bg_geo, by = join_by(bg_string == GEOID))

#canopy theme
map_2020 <- canopy_total %>%
  filter(score >= 1) %>% 
  left_join(bg_geo, by = join_by(bg_string == GEOID))

#health theme
map_2020 <- health_total %>%
  filter(score >= 1) %>% 
  left_join(bg_geo, by = join_by(bg_string == GEOID))

#social indicators theme (ignoring no more than 3 points from race vars rule)
map_2020 <- eq_total %>%
  filter(score >= 2) %>% 
  left_join(bg_geo, by = join_by(bg_string == GEOID))

plot(bg_geo$geometry)
plot(map_2020$geometry, col = "lightblue", add=T)

plot(bg_geo$geometry)
plot(tst$geometry, col = "lightblue", add=T)
```

#graphing test
```{r}

data <- mn_bgs %>% 
  filter(jurisdiction == "Madison")

b <- select(data, as.name(theme_1)) %>% 
  st_drop_geometry()
  
c <- select(data, as.name(theme_2)) %>% 
  st_drop_geometry() 

a <- xyTable(b[[1]], c[[1]]) %>% 
  as.data.frame()

d <- b %>% group_by(Temperature) %>%  summarise(n()) %>% mutate(y = 0) %>% rename()
 

ggplot(d, aes(x=Temperature, y=`n()`)) +
  geom_segment( aes(x=Temperature, xend=Temperature, y=0, yend=`n()`), color="skyblue") +
  geom_point( color="blue", size=4, alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )

  #this one!
ggplot(d, aes(x=Temperature, y=y, size=`n()`)) + 
    geom_point(alpha=0.5, shape=21, color="#77a12e", fill="#77a12e") +
  scale_size(range = c(5, 15), name="Population (M)") +
  scale_y_continuous(n.breaks = 3) +
  scale_x_continuous(n.breaks = 2) +
  geom_text(aes(Temperature, y, label = `n()`, size = .1)) +
  theme(legend.position="none", 
            rect = element_blank(),
            #panel.grid = element_blank(),
            axis.title.y = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y=element_blank()
            ) +
    coord_fixed(ratio = 1)



 ggplot(a, aes(x=x, y=y, size=number, fill=number)) +
    geom_point(alpha=0.5, shape=21, color="black") +
    scale_size(range = c(.1, 24), name="Population (M)") +
    scale_fill_viridis_b() +
    theme(legend.position="bottom") +
    ylab("Life Expectancy") +
    xlab("Gdp per Capita") +
    theme(legend.position = "none") %>% 
   
 
 
ggplot(data, aes(x=`Health Disparities`, y=`Canopy Cover`)) + 
    geom_jitter(width = 0.1, height = 0.1, alpha = .5) 
  

b <- uncount(a, number)

ggplot(b, aes(x=x, y=y) ) +
  geom_hex(bins = 30) +
  scale_fill_continuous(type = "viridis") +
  theme_bw()

a <- tolower(theme_1) %>% 
 gsub(" ", "_", .)

c <- metadata %>% 
  select(as.name(a)) %>% 
  sum()
```

